# Mapping Quality Assurance Exercise

- Introduces the 2007 Countryside Survey digital mapping and QA process to ensure high-quality, field-macred data compatible with national datasets.
- Uses Surveyor software for in-field mapping with mandatory fields, prompts, visit-status layers, and wiki-based early data checks by office staff.
- QA (quality assurance) compares field data collected by surveyors (CS) with QA datasets to assess accuracy and consistency across habitats, features, and change recordings.

## Scope and Extent

- QA conducted across 23 1km squares, representing diverse land classes to assess data quality and highlight potential biases.
- Data processed as two comparable geodatabases (surveyors vs QA) with unsurveyed or access-refused areas excluded from analyses.

## QA Approaches and Methods

- Direct comparison of aggregate areas/lengths/points for whole squares (CS vs QA).
- Raster data comparisons: converting polygons to 10 x 10 m raster cells per 1 km square to compare dominant Broad Habitats and their locations.
- 100-point grid overlays to assess concordance of Broad Habitats at a standardized resolution.
- Attribute-level analyses: creating reference ID layers to compare species attributes where available.
- Documentation of surveyor efficiency via visit status and species recording, with emphasis on data capture consistency.

## Surveyor Efficiency and Data Capture

- Digital data collection improved coverage and reduced non-visited areas; average non-visited area across Squares stayed very low (often near 0%).
- Mandatory fields led surveyors to record more species, expanding the richness of field data.

## 6. Habitat and Area Comparisons

- 6.1 Direct square comparisons
  - In 81% of squares, both CS and QA recorded the presence of Broad Habitats (BH).
  - Most mean differences in BH extents were under 1â€“2%; a few habitats showed larger differences (e.g., Improved grassland, Dwarf shrub heath, Bog, Urban, Sub-littoral sediment).
  - Blanket Bog inconsistencies linked to definitional debates; upland mosaics and land-use mosaics posed matching challenges.
- 6.2 Raster data comparisons
  - Overall raster agreement varied by square; high concordance in several squares (e.g., 364, 488) but substantial discordance in others (e.g., 773, 261).
  - Agreement by BH per square highlighted both strong matches and notable misclassifications across some habitat types.
- 6.3 100-point grid concordance
  - Generally good agreement across squares, but notable mismatches in a few (e.g., squares 773 and 261).
  - Matrix analysis showed systematic differences in certain habitat codes, with some BHs more prone to misassignment.
- 6.4 Polygon species attributes
  - 1,081 QA polygons and 998 CS polygons spatially matched; about 45% had at least one matching listed species.
  - Species concordance varied by habitat type; higher for woodlands and Improved Grassland, lower for Bracken and upland mosaics.
  - In 77% of polygons with a species match, the same habitat had a match, indicating species and habitat alignment is more variable than location alone.
- 6.5 Species lists
  - Overall concordance around 40% for commonly recorded species; QA tended to record more species than CS.
  - Dominant species like Lolium perenne exhibited relatively higher agreement than others.

## 7. Linear Features

- 7.1 Aggregate lengths
  - Lengths of linear features showed minor differences in most squares, with some variation across land-use themes (banks, fences, forestry belts, etc.).
- 7.2 Land-use theme comparisons
  - High consistency in the distribution of land-use themes for linears; few cases where QA recorded a feature not in CS, mainly due to feature-type confusion (e.g., fence vs wall).
  - Some complexity between forestry belts (FO) and woody linear features (WNS, WUS), but overall consistency was strong.

## 8. Points

- 8.1 Aggregate points
  - Differences in total points per square were small across most squares.
- 8.2 Point habitat codes
  - High consistency in point feature habitat coding; minor confusion around woodland/forest codes (likely legacy issues).

## 9. Change Recording

- 2007 introduced more detailed change coding: Real change, Error Change, No change.
- Change field use proved complex; some surveyors struggled initially, but overall agreement between CS and QA was good.
- Tables show strong agreement for many habitat changes and linear/point changes, with habitat-change assessment more challenging than linear or point changes due to interpretation.

## Key Findings and Implications for Data Support

- Overall data quality is solid, with strong cross-checks between field and QA data demonstrating reliable mapping and attribute recording.
- Definitional and contextual issues (notably Blanket Bog, conifer vs broadleaved woodland, and upland mosaic mapping) drive some discrepancies; targeted refinement of habitat definitions and field-handbook guidance is recommended.
- Mosaic and small-scale variability in upland habitats can produce mis-matches; enhanced training and harmonization of mapping approaches will help reduce these.
- The digital workflow (Surveyor application and wiki-based QA) improved data tidiness, logging of change, and the ability to perform in-depth QA analyses in a timely manner.
- For habitat categories with overlapping species and structural characteristics (e.g., Neutral vs Improved Grassland), discrepancies arose from interpretation; ongoing refinement of allocation rules is advised.
- The change-recording framework, while complex, enabled better data updating and correction workflows, though continued training and clarity in change definitions are beneficial.

## Conclusions

- The 2007 digital mapping QA exercise demonstrates a high level of agreement between surveyors and QA teams across most habitat classes, linear features, and point features.
- Where disagreements occur, they are explainable by definitional issues, mosaic mapping in upland areas, and local habitat peculiarities (e.g., machair, Blanket Bog).
- Recommendations include refining habitat definitions, providing targeted training to survey teams, continuing to use digital change fields for data updates, and maintaining robust QA processes to support future surveys and data integration across national datasets.