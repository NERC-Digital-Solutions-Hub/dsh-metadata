# Background

- Purpose: Establish a standard to assess the quality of the National Plant Monitoring Scheme (NPMS) as a source of scientific advice and evidence fit for purpose.
- Compliance: Aligns with The Government Chief Scientific Adviser's Guidelines on the Use of Scientific and Engineering Advice in Policy Making (2010), The Defra Joint Code of Practice for Research (2012), and JNCC EQA Guidance notes.
- Evidence, QA concepts: Defines evidence broadly (opinion, data, methodology, results, interpretations) and QA as processes ensuring work meets quality standards, with monitoring and auditing as core components; two QA principles are “Fit for purpose” and “Right first time.”

- NPMS objectives:
  - Detect change in the quality of priority habitats.
  - Understand why change is occurring (pressures and drivers).
  - Contribute data for UK Biodiversity Indicator C7 (Plants of the wider countryside) by measuring plant community change across key habitats.

- Application of EQA to NPMS: Summarises how quality assurance questions are addressed through NPMS processes, outputs, and governance, including design, data handling, and reporting.

## Evidence and outputs supporting EQA

- Published peer-reviewed journal articles
  - Example: Pescott et al. 2015 on ecological monitoring with citizen science and plant recording in Britain and Ireland.
- Papers in preparation
  - Comparisons of models for interval-censored plant cover data; design of a new UK plant monitoring scheme (prep work).
- Other published articles
  - Articles in British Wildlife and newsletters detailing NPMS development and status.
- Reports (internal and external)
  - Design of a new plant surveillance scheme; development work for NPMS (2014–2015); cross-scheme communications and uptake considerations.

## QA questions and how NPMS addresses them (Q1–Q5)

- Q1. Partnership peer review of methods and publications
  - Design and methods developed via expert workshop (statistical and botanical).
  - Flow: method development → data analysis testing → field testing with volunteers → training and questionnaires; final plot design uses a regular grid with random-like placement to reduce bias.
  - Habitat definitions and target species selections reviewed by habitat experts and project team; field recording designed for volunteer participation with training; data input uses a structured online system with a species dictionary, dates, locations, and controlled terms.
  - Data verification: automated checks (geographic range, etc.) and expert review of exceptions; data publishing planned for public access; analysis by quantitative ecologists using R; reporting by a multidisciplinary project team; dissemination through NPMS newsletter and peer-reviewed journals.
- Q2. Appropriate level of review/quality assurance by process/outputs
  - Managed by a project Steering Group; external peer review engaged when risk warrants it (e.g., survey design workshop).
- Q3. Communicating uncertainty, quality, assumptions, limitations
  - Outputs reviewed by scientists to ensure claims are evidence-supported; ongoing external feedback from ecologists within CEH to improve interpretation and transparency.
- Q4. Avoiding biases in statistics and other components
  - Robust statistical design enabling uncertainty estimation through modelling.
- Q5. Document tracking and version control
  - Design work fully reported; versions archived and backed up off-site per CEH Data Management Plan; project managers follow PRINCE2 standards.

## Data handling and analytical workflow (NPMS specifics)

- Field recording: volunteer-led with three participation levels; training provided; categorisation and abundance recording guided by manuals.
- Data input and verification: online, structured spatial database; validation via species dictionary, date capture, location mapping, and habitat/vegetation descriptors; automated and expert verification of exceptions.
- Data publishing: aims to publicly share data for independent review.
- Analysis: conducted by experienced quantitative ecologists using R; results interpreted and reported by a diverse project team (botany, statistics, communications).
- Reporting: disseminated through newsletters and peer-reviewed journal publications; emphasis on clear communication to researchers, volunteers, policymakers, and the public.

## Governance, documentation, and future developments

- Oversight: NPMS project Steering Group; risk-based external peer reviews as needed; workshop-type expert input for design issues.
- Documentation and version control: all design outputs tracked; versions archived; off-site backups; adherence to a CEH Data Management Plan and PRINCE2 standards.
- Annex 1 (methodological paper outline)
  - Sections cover: Introduction (need for structured plant monitoring data and comparisons with other taxa); Rationale and aims; Design (sampling framework, habitat/species selection, field methods, plot details, timing, visits); Maximising volunteer participation (recruitment, training, coordination); Discussion (initial evaluation, participation levels, remote areas, future plans).
  - Indicates ongoing development and peer review opportunities to address gaps in EQA.

## Key takeaways for data-focused analysts

- NPMS is designed with a formal QA framework aligned to government and research standards.
- Data collection, input, verification, and publication workflows are codified to support transparency and reproducibility.
- Uncertainty is explicitly contemplated through robust statistical design and model-based inference.
- Governance structures ensure risk-based external review and method transparency, with comprehensive documentation and version control.
- The methodological framework is iterative, with ongoing opportunities for peer review and methodological enhancements.